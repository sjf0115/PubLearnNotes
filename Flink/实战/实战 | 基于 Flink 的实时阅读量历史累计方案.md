
## 1. 背景

客户提出了实时展示文章(视频)阅读量的诉求，对于图文使用文章的浏览次数 PV 指标计算，而视频则使用视频的播放次数 VV 计算。

业务需求本质是实时计算出每个内容自上线开始到当前时刻累计浏览次数 PV 和 累计播放次数 VV 指标，具体展示 PV 还是 VV 视具体的内容类型(是图文还是视频)。

![](/neweditor/e9166be4-1e46-4a96-a7c6-cbc5ea7ad1b4.png)

比如说这里的 4567554 表示该内容从上线后到当前时刻的阅读量，由于是一个图文内容，所以实际上是自上线后的历史累计浏览次数。


## 2. 解决方案

对于实时计算，我们计算任务是时刻运行的；对于指标的逻辑处理，计算的状态不能丢失；因此，不同于离线批处理的逻辑；实时当中始终要考虑的：
- 故障恢复；
- 实现成本
- 运维成本；

以上几点可能直接影响最终方案的选择。我们希望实时任务的处理逻辑简单，维护的状态要小，甚至不需要状态计算。这样，可以减少任务运维出错的几率，此外更重要的是我们不可能维护一个计算历史累计的实时任务，如果状态丢失不可能从历史上线的那一刻重新回溯数据（消息中间件也不支持我们回溯那么久的数据）。因此我们希望，实时只处理当前很小的时间粒度上的指标，例如零点到当前时刻的累计值。这样实时任务维护的状态不至于很大，时间也不会太长。我们依赖离线任务完成从上线以来截至到昨日的历史累计量；最终通过一个实时任务或者服务做一次聚合就可以了。这本质上是一个典型的 Lambda 架构的服务。


![](/neweditor/d13dcbd9-d692-45ba-8072-50c77d401940.png)

我们将整个服务拆分为实时+离线两条链路分别计算，离线链路计算历史截止到昨日的累计量，实时链路计算从零点截止到当前时刻的累计量。最后在融合部分对两条链路的累计量进行汇总，即从上线开始截止到当前时刻的历史累计量。

### 2.1 离线部分

离线部分由 MaxCompute 负责计算每个内容在 T 日的累计浏览次数 PV 和累计播放次数 VV。再累加昨天生成的每个内容截止到 T-1 日的历史累计浏览次数 PV 和历史累计播放次数 VV，生成最新的截止到 T 日的历史累计浏览次数 PV 和历史累计播放次数 VV。经过离线 ETL 处理之后，最终通过数据集成同步到 Redis 中供实时任务使用。

![](/neweditor/d01266b8-16ed-49a0-b479-b3e8c27d0c21.png)

Redis 数据存储格式如下：

| 存储结构 | 存储类型 | 格式 | 说明 |
| -------- | -------- | -------- | -------- |
| Hash     | Key     | `{item_id}_{yyyyMMdd}_td` | 某个内容在某天的历史累计量，例如 11927359315533291687_20220925_td |
| Hash     | Field   | PV | 历史累计浏览次数 |
| Hash     | Field   | VV | 历史累计播放次数 |

离线部分的历史累计量同步到 Redis 中会作为第二个实时任务 Blink Stream 的维表使用。

需要注意的是，由于离线延迟产出的特性，截止到 T 日的历史累积量需要在 T+1 日的某个时间点生成，例如3点。也就是3点之前，截止到 T 日的历史累积量还没产出，即 Redis 中只有截止到 T-1 日的历史累计量。这一点会影响到融合部分如何计算最终的结果值。

### 2.2 实时部分

实时部分任务是 Blink SQL 任务，负责计算当日从零点截止到当前时刻的累计浏览次数和累计播放次数。

Blink SQL 任务实时消费 SLS 中的 dwd_app_page_view_rt 浏览明细数据以及 dwd_app_play_rt 播放明细数据。然后实时计算零点截止到当前的累积量结果写入 SLS 中的 dws_app_content_visit_rt 阅读量汇总层，同时也会把结果更新到 Redis 中（在后续计算作为维表使用）。

![](/neweditor/b2fdcca0-a9cb-4bbd-9d69-bd61a8f585fa.png)

Redis 中的存储格式跟 MaxCompute 同步到 Redis 中的几乎一样，只是 Key 不再是以 td 结尾，而是以 1d 结尾：

| 存储结构 | 存储类型 | 格式 | 说明 |
| -------- | -------- | -------- | -------- |
| Hash     | Key     | `{item_id}_{yyyyMMdd}_1d` | 某个内容从零点截止到当前时刻的累计量，例如格式为 11927359315533291687_20220925_1d |
| Hash     | Field   | PV | 从零点截止到当前时刻的累计浏览次数 |
| Hash     | Field   | VV | 从零点截止到当前时刻的历史累计播放次数 |


需要注意的是 Blink SQL 任务是一个有状态的任务，在状态 State 中存储从零点截止到当前时刻的累计浏览次数和累计播放次数。如果需要重启任务，必须从重置到零点的 Offset 偏移量重新消费数据，势必会导致线上断流或者数据指标的回溯(回溯到之前某一个时间点的值)。

### 2.3 融合部分

融合部分是一个 Blink Stream 任务，负责实时消费 SLS 中阅读量汇总层 dws_app_content_visit_rt 的 0 点截至当前时刻的累积量实时流，同时读取 Redis 维表中的 T-1 日的历史累积量或者截至 T-2 日的历史累积量(以及 T-1 日的当日累积量，由于此时 T-1 日的历史累积量还未产出)。通过该任务完成几部分数据的实时加和，并把最终结果通过 Rpc 接口写到信息流计数器服务中。

在上述离线部分，我们提及过离线延迟产出的特性，截止到 T 日的历史累积量需要在 T+1 日的某个时间点生成，不会在 0 点立即产出。所以在凌晨跑批更新的时候存在执行和替换的时间，如何确保每时每刻服务都能查到最全的明细数据，这是 Lambda 方案设计的关键。我们基于 Redis 设计两种类型的分区：一种是实时 Key `xxx_{yyyyMMdd}_1d`，另一种是离线 Key `xxx_{yyyyMMdd}_td`。

![](/neweditor/dc888552-c889-4fde-8c06-a64b249154ab.png)

> 假设 T 日为 20220925


具体如何计算出当前最新的历史累计量呢？当现在是 T 日，T-1 日离线数据正在执行还未产出时，如下图的 Step1（绿色部分）：
- 最新历史累计量 = T 日实时数据 + （T-1) 日实时数据 + (T-2) 日离线数据
当现在是 T 日，T-1 日离线数据已产出时，如下图的 step2（红色部分）：
- 最新历史累计量 = T 日实时数据 + （T-1) 日离线数据

具体过程如下图，何时从 Step1 切换到 Step2，我们通过在 Blink Stream 代码中判断，使用此方案能确保写入到下游 Rpc 计数器服务接口都是最新的全量数据。对于实时 Key 和离线 Key 分别都配置 3 天有效期，过期失效的历史分区会自动清理。

![](/neweditor/a2a51b1a-36ba-42e8-b786-68302c2d0617.png)

其实我们可以看到 Blink Stream 任务也是一个 Blink 实时任务，为什么没有合在一个任务中计算呢？拆分为两个任务的原因如下几个：
- Blink SQL 任务是有状态任务，重启需要重置到零点的消费位点，运维复杂，不会轻易变动任务；而 Blink Stream 任务是无状态任务，重启不需要重置位点，运维简单，可以随意的上下线。
- Blink SQL 任务逻辑简单，只是计算当日的累计浏览次数 PV 和累计播放次数 VV；而 Blink Stream 任务与 Redis 交互频繁，并且需要将结果写到下游的 Rpc 服务中，使用 DataStream 方式方便一些。

## 3. 稳定性保障

稳定性保障的一个核心的问题就是有状态的 Blink SQL 逻辑变更后状态如何恢复？Blink SQL 支持有状态的增量计算，状态是增量计算的历史累计，实际上业务需要修改逻辑的情况很多，比如修改原指标口径、增加过滤条件等。

举个例子，业务增加了更加严格的限定，因此也需要修改 Blink SQL 增加过滤条件。但是上述 SQL 逻辑变化后却不能从之前的状态恢复(拓扑图发生了变化)，因为历史状态对于变更后的 SQL 不能保证其完整性，即使恢复后也不能百分百保证后续计算的正确性。这种情况下，业务为了保证数据的正确性，需要从历史回溯重新计算，回溯的过程会导致线上断流，但业务又不希望牺牲太多的时效性。

针对这个问题，我们使用双链路切换的方式来保障。双链路切换的关键是再搭建一条相同的实时链路作为备用链路，当变更有状态的 Blink SQL 任务时，可以在备用链路上做回溯，重新计算历史数据，回溯完成后先验证备用链路的结果数据，确保没问题后再在链路最下游的数据服务层切换读取的上游，完成整个变更流程。

![](/neweditor/3f857891-55e6-4396-9635-ab58872a1f35.png)

Blink Stream 任务会添加一个输出计数器服务的开关。每次新上线都会开发一套影子任务(备用链路)，相比于线上任务的区别是添加要上线的业务逻辑以及关闭输出计数器服务的开关。影子任务和线上任务都会输出日志到SLS，通过对比两份SLS日志来判断影子任务跟线上任务的输出结果是否一样（至少差距比较小）来判断影子任务是否跟线上任务整体逻辑保持一致。

在影子任务上做回溯，重新计算历史数据，回溯完成之后就可以选择替换主链路的线上任务。甚至可以让影子任务运行一段时间观察是否运行稳定。上线影子任务之前，需要先关闭线上任务写计数器服务的开关。关闭线上计数器服务开关之后，就可以打开影子任务的写计数器服务的开关来完成上线。

> 也可以利用影子任务来模拟上线过程，排查上线是否会对数据结果产生影响。
